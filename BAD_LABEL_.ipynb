{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bad labels: dataset**"
      ],
      "metadata": {
        "id": "n-UvGniBxQZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many popular datasets like MNIST and CIFAR have incorrect labels that can affect machine learning benchmarks. This problem is detailed on labelerrors.com and in the research paper available on arXiv. We'll use cleanlab to identify and correct these errors in our training data."
      ],
      "metadata": {
        "id": "FA4-psv3yKvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GoEmotions dataset, created by Google, contains text snippets from Reddit annotated with emotion labels. It's designed for emotion prediction tasks, focusing on labels like \"excitement\". You can download the dataset using the following commands:"
      ],
      "metadata": {
        "id": "TyQ5FVxpytfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with a simple model using the GoEmotions dataset"
      ],
      "metadata": {
        "id": "ZGTZR6gY1IOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Dataset:"
      ],
      "metadata": {
        "id": "PXcCWOBf1dGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# File path\n",
        "file_path = 'goemotions_1.csv'\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.isfile(file_path):\n",
        "    print(f\"File not found: {file_path}. Downloading...\")\n",
        "    # Make sure the data/full_dataset/ directory exists\n",
        "    os.makedirs('data/full_dataset/', exist_ok=True)\n",
        "    # Command to download the file\n",
        "    os.system(f'wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/{file_path}')\n",
        "    file_path = f'data/full_dataset/{file_path}'  # Update file path after download\n",
        "\n",
        "# Now load the data\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"File loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "Ho73N52m4OD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preview the Data:"
      ],
      "metadata": {
        "id": "zLDhucIf4kIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "print(df[['text', 'excitement']].loc[lambda d: d['excitement'] == 0].sample(2))\n"
      ],
      "metadata": {
        "id": "yMo7n9w44pWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle Class Imbalance:"
      ],
      "metadata": {
        "id": "uuZoHfy84tQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['excitement'].value_counts())\n"
      ],
      "metadata": {
        "id": "F9-lbvv64wfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and Train the Model:"
      ],
      "metadata": {
        "id": "j3bScgsA42EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "X, y = df['text'], df['excitement']\n",
        "pipe = make_pipeline(\n",
        "    CountVectorizer(),\n",
        "    LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        ")\n",
        "pipe.fit(X, y)\n"
      ],
      "metadata": {
        "id": "Bl2Fd5BR48zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Bad Labels using Model Disagreement"
      ],
      "metadata": {
        "id": "_w_Nwp8P5w3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"goemotions_1.csv\")\n",
        "\n",
        "# Setup DataFrame to display full text\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Create a pipeline with logistic regression and a count vectorizer\n",
        "X, y = df['text'], df['excitement']\n",
        "pipe = make_pipeline(\n",
        "    CountVectorizer(),\n",
        "    LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        ")\n",
        "pipe.fit(X, y)\n",
        "\n",
        "# Function to calculate the confidence of the correct class\n",
        "def correct_class_confidence(X, y, model):\n",
        "    probas = model.predict_proba(X)\n",
        "    values = [proba_dict[y[i]] for i, proba_dict in enumerate(map(lambda p: dict(zip(model.classes_, p)), probas))]\n",
        "    return values\n",
        "\n",
        "# Assign confidence values to the DataFrame\n",
        "df['confidence'] = correct_class_confidence(X, y, pipe)\n",
        "\n",
        "# Filter out examples where predictions do not match the labels\n",
        "disagreements = df.loc[lambda d: pipe.predict(d['text']) != d['excitement']]\n",
        "disagreements = disagreements.assign(confidence=correct_class_confidence(disagreements['text'], disagreements['excitement'], pipe))\n",
        "\n",
        "# Sort by confidence and filter for specific examples\n",
        "sorted_disagreements = (disagreements\n",
        "                        .loc[lambda d: d['excitement'] == 0]\n",
        "                        .sort_values(\"confidence\")\n",
        "                        .head(20))\n",
        "\n",
        "print(sorted_disagreements[['text', 'excitement', 'confidence']])\n"
      ],
      "metadata": {
        "id": "mQAaaDG-539K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruning with Cleanlab"
      ],
      "metadata": {
        "id": "T5ckPaHp6YaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from cleanlab.pruning import get_noise_indices\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"goemotions_1.csv\")\n",
        "\n",
        "# Setup DataFrame to display full text\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Create a pipeline with logistic regression and a count vectorizer\n",
        "X, y = df['text'], df['excitement']\n",
        "pipe = make_pipeline(\n",
        "    CountVectorizer(),\n",
        "    LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        ")\n",
        "pipe.fit(X, y)\n",
        "\n",
        "# Generate probabilities of each class for the dataset\n",
        "probabilities = pipe.predict_proba(X)\n",
        "\n",
        "# Use cleanlab to identify potential label issues\n",
        "ordered_label_errors = get_noise_indices(\n",
        "    s=y,\n",
        "    psx=probabilities,\n",
        "    sorted_index_method='prob_given_label'\n",
        ")\n",
        "\n",
        "# Display potential mislabeled examples\n",
        "mislabeled_examples = df.iloc[ordered_label_errors][['text', 'excitement']].head(20)\n",
        "print(mislabeled_examples)\n"
      ],
      "metadata": {
        "id": "ia-Hpl6x6ZZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning with Noisy Labels via Cleanlab"
      ],
      "metadata": {
        "id": "Xqko__2_628-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from cleanlab.classification import LearningWithNoisyLabels\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"goemotions_1.csv\")\n",
        "\n",
        "# Set up the DataFrame to display the full text\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Prepare data\n",
        "X, y = df['text'], df['excitement']\n",
        "\n",
        "# Define the base pipeline with Logistic Regression\n",
        "base_pipe = make_pipeline(\n",
        "    CountVectorizer(),\n",
        "    LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        ")\n",
        "\n",
        "# Train the base pipeline\n",
        "base_pipe.fit(X, y)\n",
        "\n",
        "# Wrap the base pipeline with LearningWithNoisyLabels\n",
        "lnl_model = LearningWithNoisyLabels(clf=base_pipe)\n",
        "lnl_model.fit(X=X, s=y.values)\n",
        "\n",
        "# Compare predictions from LearningWithNoisyLabels and the base pipeline\n",
        "discrepancies = df.loc[lnl_model.predict(X) != base_pipe.predict(X)][['text', 'excitement']].sample(5)\n",
        "print(\"Discrepancies found:\")\n",
        "print(discrepancies)\n"
      ],
      "metadata": {
        "id": "drmrqm1T63_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improving a model by tuning hyperparameters can risk overfitting on bad labels. It's often more effective to first identify and correct bad labels in the dataset to ensure true model accuracy."
      ],
      "metadata": {
        "id": "qG8qBlS47P7u"
      }
    }
  ]
}